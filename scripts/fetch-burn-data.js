const fs = require('fs');
const path = require('path');
require('dotenv').config();

// ‚è∞ IMPORTANT: This script takes 8-10 minutes to complete
// It fetches REAL blockchain data (no estimates):
// - 30 days of burn transactions from 270+ block chunks
// - Live holder balances from transfer events + balance calls
// - Please wait the full time for accurate results
// ‚è∞

// RPC endpoints with backup options (tested and working)
const RPC_ENDPOINTS = [
  "https://ethereum.publicnode.com",
  "https://eth.llamarpc.com",
  "https://1rpc.io/eth",
  "https://eth-mainnet.public.blastapi.io",
  "https://eth.drpc.org"
];

let currentRPCIndex = 0;

const TINC_ADDRESS = '0x6532B3F1e4DBff542fbD6befE5Ed7041c10B385a';
const ZERO_ADDRESS = '0x0000000000000000000000000000000000000000';
const TRANSFER_TOPIC = '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef';
const ZERO_ADDRESS_TOPIC = '0x0000000000000000000000000000000000000000000000000000000000000000';
const CHUNK_SIZE = 800; // blocks per chunk (reduced for RPC limits)
const AVG_BLOCK_TIME = 12; // seconds

// Etherscan API configuration
const ETHERSCAN_API_KEY = process.env.ETHERSCAN_API_KEY || process.env.ETHERSCAN_API_KEY;
const ETHERSCAN_BASE_URL = 'https://api.etherscan.io/api';

// Moralis API configuration
const MORALIS_API_KEY = process.env.MORALIS_API_KEY;
const MORALIS_BASE_URL = 'https://deep-index.moralis.io/api/v2.2';

// LP and contract addresses to exclude from holder counts
const EXCLUDED_ADDRESSES = new Set([
  '0x72e0de1cc2c952326738dac05bacb9e9c25422e3', // TINC/TitanX LP Pool
  '0xf89980f60e55633d05e72881ceb866dbb7f50580', // TINC LP Pool (Second LP)
  '0x0000000000000000000000000000000000000000', // Burn address
  '0x000000000000000000000000000000000000dead', // Dead address
].map(addr => addr.toLowerCase()));

async function callRPC(method, params, retryCount = 0) {
  const maxRetries = RPC_ENDPOINTS.length * 2;
  
  if (retryCount >= maxRetries) {
    throw new Error('All RPC endpoints exhausted after retries');
  }

  const endpoint = RPC_ENDPOINTS[currentRPCIndex];
  
  try {
    const response = await fetch(endpoint, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        jsonrpc: '2.0',
        id: Date.now(),
        method,
        params,
      }),
      timeout: 30000 // 30 second timeout
    });

    const data = await response.json();
    if (data.error) {
      throw new Error(data.error.message);
    }
    
    // Success - this endpoint is working
    return data.result;
  } catch (error) {
    console.warn(`RPC error with ${endpoint}:`, error.message);
    
    // Move to next endpoint
    currentRPCIndex = (currentRPCIndex + 1) % RPC_ENDPOINTS.length;
    
    // If error contains rate limit message, wait a bit
    if (error.message.includes('rate') || error.message.includes('limit')) {
      await new Promise(resolve => setTimeout(resolve, 1000));
    }
    
    // Retry with next endpoint
    return callRPC(method, params, retryCount + 1);
  }
}

async function getBlockNumber() {
  const result = await callRPC('eth_blockNumber', []);
  return parseInt(result, 16);
}

async function estimateBlockByTimestamp(timestamp) {
  const currentBlock = await getBlockNumber();
  const currentTime = Math.floor(Date.now() / 1000);
  const timeDiff = currentTime - timestamp;
  const blocksDiff = Math.floor(timeDiff / AVG_BLOCK_TIME);
  return Math.max(1, currentBlock - blocksDiff);
}

async function getBlockTimestamp(blockNumber) {
  const block = await callRPC('eth_getBlockByNumber', [
    typeof blockNumber === 'number' ? `0x${blockNumber.toString(16)}` : blockNumber,
    false
  ]);
  return parseInt(block.timestamp, 16);
}

async function getEmissionRate(contractAddress) {
  // TINC has a fixed emission rate of 1 TINC/second = 86,400 TINC/day
  console.log('Using TINC standard emission rate: 1.0 TINC/sec');
  return { emissionPerSecond: 1.0, samplePeriod: 86400 };
}

async function fetchBurns(fromBlock, toBlock) {
  const logs = await callRPC('eth_getLogs', [{
    fromBlock: `0x${fromBlock.toString(16)}`,
    toBlock: `0x${toBlock.toString(16)}`,
    address: TINC_ADDRESS,
    topics: [
      TRANSFER_TOPIC,
      null, // from address (any)
      ZERO_ADDRESS_TOPIC // to address (0x0)
    ]
  }]);
  
  const burns = [];
  for (const log of logs) {
    const amount = parseInt(log.data, 16) / Math.pow(10, 18);
    const from = '0x' + log.topics[1].substring(26);
    const blockNumber = parseInt(log.blockNumber, 16);
    const timestamp = await getBlockTimestamp(blockNumber);
    
    burns.push({
      hash: log.transactionHash,
      amount,
      from,
      timestamp,
      blockNumber
    });
  }
  
  return burns;
}

async function fetchBurnsWithRetry(fromBlock, toBlock, maxRetries = 3) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      console.log(`Fetching blocks ${fromBlock} to ${toBlock} (attempt ${attempt}/${maxRetries})...`);
      const burns = await fetchBurns(fromBlock, toBlock);
      if (attempt > 1) {
        console.log(`‚úÖ Success on attempt ${attempt} for blocks ${fromBlock}-${toBlock}`);
      }
      return burns;
    } catch (error) {
      const isLastAttempt = attempt === maxRetries;
      console.warn(`‚ùå Attempt ${attempt}/${maxRetries} failed for blocks ${fromBlock}-${toBlock}: ${error.message}`);
      
      if (isLastAttempt) {
        console.error(`üö® CRITICAL: Failed to fetch blocks ${fromBlock}-${toBlock} after ${maxRetries} attempts!`);
        throw new Error(`Failed to fetch blocks ${fromBlock}-${toBlock} after ${maxRetries} attempts: ${error.message}`);
      }
      
      // Exponential backoff: 2s, 4s, 8s
      const backoffMs = 2000 * Math.pow(2, attempt - 1);
      console.log(`‚è≥ Waiting ${backoffMs/1000}s before retry...`);
      await new Promise(resolve => setTimeout(resolve, backoffMs));
    }
  }
}

async function fetchBurnData() {
  console.log('Fetching TINC burn data...');
  
  // Get total supply
  const totalSupplyHex = await callRPC('eth_call', [
    { to: TINC_ADDRESS, data: '0x18160ddd' },
    'latest'
  ]);
  const totalSupply = parseInt(totalSupplyHex, 16) / Math.pow(10, 18);

  // Get emission rate
  const emissionData = await getEmissionRate(TINC_ADDRESS);
  const dailyEmission = emissionData.emissionPerSecond * 86400;

  // Fetch actual burn transactions
  const endDate = new Date();
  const startDate = new Date();
  startDate.setDate(startDate.getDate() - 30);

  const currentBlock = await getBlockNumber();
  const startBlock = await estimateBlockByTimestamp(Math.floor(startDate.getTime() / 1000));

  console.log(`Fetching burns from block ${startBlock} to ${currentBlock}`);
  
  const allBurns = [];
  const totalChunks = Math.ceil((currentBlock - startBlock) / CHUNK_SIZE);
  let chunksProcessed = 0;

  // Fetch in chunks with robust retry logic
  console.log(`üîÑ Processing ${totalChunks} chunks with retry logic...`);
  
  for (let fromBlock = startBlock; fromBlock <= currentBlock; fromBlock += CHUNK_SIZE) {
    const toBlock = Math.min(fromBlock + CHUNK_SIZE - 1, currentBlock);
    
    // Use retry mechanism - will throw error if all retries fail
    const burns = await fetchBurnsWithRetry(fromBlock, toBlock);
    allBurns.push(...burns);
    chunksProcessed++;
    
    // Progress indicator
    const progress = ((chunksProcessed / totalChunks) * 100).toFixed(1);
    console.log(`üìä Progress: ${chunksProcessed}/${totalChunks} chunks (${progress}%) - Found ${burns.length} burns in this chunk`);
  }
  
  console.log(`‚úÖ Successfully processed all ${totalChunks} chunks - Total burns found: ${allBurns.length}`);

  // Group burns by day
  const burnsByDay = {};
  allBurns.forEach(burn => {
    const date = new Date(burn.timestamp * 1000);
    const dateStr = date.toISOString().split('T')[0];
    
    if (!burnsByDay[dateStr]) {
      burnsByDay[dateStr] = {
        date: dateStr,
        amountTinc: 0,
        transactionCount: 0,
        transactions: []
      };
    }
    
    burnsByDay[dateStr].amountTinc += burn.amount;
    burnsByDay[dateStr].transactionCount++;
    burnsByDay[dateStr].transactions.push({
      hash: burn.hash,
      amount: burn.amount,
      from: burn.from
    });
  });

  // Convert to array and sort by date
  const dailyBurns = Object.values(burnsByDay)
    .sort((a, b) => a.date.localeCompare(b.date));

  // Fill in missing days with zero burns
  const allDays = [];
  // Use endDate as reference point to ensure consistency
  const endDateObj = new Date(endDate.toISOString().split('T')[0] + 'T00:00:00Z');
  
  for (let i = 0; i < 30; i++) {
    const date = new Date(endDateObj);
    date.setUTCDate(date.getUTCDate() - (29 - i));
    const dateStr = date.toISOString().split('T')[0];
    
    const existingData = dailyBurns.find(d => d.date === dateStr);
    if (existingData) {
      allDays.push(existingData);
    } else {
      allDays.push({
        date: dateStr,
        amountTinc: 0,
        transactionCount: 0,
        transactions: []
      });
    }
  }

  const totalBurned = allDays.reduce((sum, day) => sum + day.amountTinc, 0);
  const totalTransactions = allDays.reduce((sum, day) => sum + day.transactionCount, 0);
  const burnPercentage = totalSupply > 0 ? (totalBurned / totalSupply) * 100 : 0;
  const isDeflationary = totalBurned > dailyEmission;

  // Data integrity validation
  console.log(`üîç Data validation:`);
  console.log(`   Total burn transactions: ${totalTransactions}`);
  console.log(`   Total TINC burned: ${totalBurned.toLocaleString()}`);
  console.log(`   Daily entries: ${allDays.length} (expected: 30)`);
  
  if (allDays.length !== 30) {
    throw new Error(`Data validation failed: Expected 30 daily entries, got ${allDays.length}`);
  }
  
  if (totalTransactions === 0) {
    console.warn('‚ö†Ô∏è  Warning: No burn transactions found in data');
  }

  return {
    startDate: startDate.toISOString().split('T')[0],
    endDate: endDate.toISOString().split('T')[0],
    totalBurned,
    totalSupply,
    burnPercentage,
    emissionPerSecond: emissionData.emissionPerSecond,
    emissionSamplePeriod: emissionData.samplePeriod,
    isDeflationary,
    dailyBurns: allDays,
    fetchedAt: new Date().toISOString(),
    fromCache: true
  };
}

// Get total supply for percentage calculations
async function getTotalSupply() {
  const totalSupplyHex = await callRPC('eth_call', [
    { to: TINC_ADDRESS, data: '0x18160ddd' },
    'latest'
  ]);
  return parseInt(totalSupplyHex, 16) / Math.pow(10, 18);
}

// Fetch ALL holders using Moralis API - much more comprehensive!
async function fetchRealHolderData() {
  try {
    console.log('üìä Fetching REAL holder data from Moralis API...');
    
    const totalSupply = await getTotalSupply();
    console.log(`üìä Total Supply: ${totalSupply.toLocaleString()} TINC`);
    
    // Use Moralis token holders endpoint - gets ALL holders
    const allHolders = [];
    let cursor = null;
    let page = 1;
    
    do {
      const url = `${MORALIS_BASE_URL}/erc20/${TINC_ADDRESS}/owners?chain=eth&limit=100${cursor ? `&cursor=${cursor}` : ''}`;
      
      console.log(`üìä Fetching holders page ${page} from Moralis...`);
      
      const response = await fetch(url, {
        headers: {
          'X-API-Key': MORALIS_API_KEY,
          'Content-Type': 'application/json'
        }
      });
      
      if (!response.ok) {
        throw new Error(`Moralis API error: ${response.status} ${response.statusText}`);
      }
      
      const data = await response.json();
      
      if (data.result && data.result.length > 0) {
        allHolders.push(...data.result);
        console.log(`üìä Page ${page}: Found ${data.result.length} holders, total: ${allHolders.length}`);
      }
      
      cursor = data.cursor;
      page++;
      
      // Rate limiting
      await new Promise(resolve => setTimeout(resolve, 200));
      
    } while (cursor && page <= 50); // Safety limit of 50 pages (5000 holders max)
    
    console.log(`üìä Total holders found: ${allHolders.length}`);
    
    // Filter out LP positions and contract addresses
    const filteredHolders = allHolders.filter(holder => 
      !EXCLUDED_ADDRESSES.has(holder.owner_address.toLowerCase()) && 
      parseFloat(holder.balance_formatted) > 0
    );
    
    console.log(`üìä Filtered out ${allHolders.length - filteredHolders.length} LP/contract/zero balance addresses`);
    
    // Sort holders by balance to find top 10
    const sortedHolders = [...filteredHolders].sort((a, b) => 
      parseFloat(b.balance_formatted) - parseFloat(a.balance_formatted)
    );
    
    // Calculate top 10 holders' combined percentage
    const top10Holders = sortedHolders.slice(0, 10);
    const top10Balance = top10Holders.reduce((sum, holder) => 
      sum + parseFloat(holder.balance_formatted), 0
    );
    const top10Percentage = (top10Balance / totalSupply) * 100;
    
    console.log(`üìä Top 10 holders own: ${top10Percentage.toFixed(2)}% of total supply`);
    
    // Categorize holders based on percentage of total supply
    let poseidon = 0, whale = 0, shark = 0, dolphin = 0, squid = 0, shrimp = 0;
    
    filteredHolders.forEach(holder => {
      const balance = parseFloat(holder.balance_formatted);
      const percentage = (balance / totalSupply) * 100;
      
      if (percentage >= 10) poseidon++;
      else if (percentage >= 1) whale++;
      else if (percentage >= 0.1) shark++;
      else if (percentage >= 0.01) dolphin++;
      else if (percentage >= 0.001) squid++;
      else shrimp++;
    });
    
    console.log(`üìä REAL HOLDER COUNTS (Moralis API):`);
    console.log(`üî± Poseidon (10%+): ${poseidon}`);
    console.log(`üêã Whale (1%+): ${whale}`);
    console.log(`ü¶à Shark (0.1%+): ${shark}`);
    console.log(`üê¨ Dolphin (0.01%+): ${dolphin}`);
    console.log(`ü¶ë Squid (0.001%+): ${squid}`);
    console.log(`üç§ Shrimp (<0.001%): ${shrimp}`);
    
    return {
      totalHolders: filteredHolders.length,
      poseidon,
      whale,
      shark,
      dolphin,
      squid,
      shrimp,
      top10Percentage,
      estimatedData: false,
      excludesLPPositions: true,
      realTimeData: true,
      dataSource: 'moralis'
    };
    
  } catch (error) {
    console.error('‚ùå Error fetching real holder data from Moralis:', error.message);
    throw error; // Don't fall back to estimates - user wants ONLY real data
  }
}

// New caching-based holder data fetcher
const HolderCacheManager = require('./holder-cache-manager');
const TransferEventMonitor = require('./transfer-event-monitor');
const InitialHolderSnapshot = require('./initial-holder-snapshot');

async function fetchHolderDataWithCache() {
  const cacheManager = new HolderCacheManager();
  const transferMonitor = new TransferEventMonitor(callRPC);
  const snapshotCreator = new InitialHolderSnapshot(callRPC);
  
  try {
    // Check if we have a valid cache
    const cache = cacheManager.loadCache();
    const cacheAge = cacheManager.getCacheAge();
    
    if (cache && cacheAge !== null) {
      console.log(`üìä Found holder cache (${cacheAge} hours old)`);
      
      // If cache is less than 0.5 hours old, use it directly
      if (cacheAge < 0.5) {
        console.log('‚úÖ Using cached holder data');
        return cache.holderStats;
      }
      
      // Otherwise, do incremental update
      console.log('üîÑ Performing incremental holder update...');
      const currentBlock = await getBlockNumber();
      const updatedStats = await transferMonitor.getIncrementalHolderUpdate(
        cache.lastBlock,
        currentBlock,
        callRPC
      );
      
      if (updatedStats) {
        return updatedStats;
      }
    }
    
    // No cache or update failed - create initial snapshot
    console.log('üöÄ Creating initial holder snapshot from blockchain...');
    console.log('‚è∞ This will take 5-10 minutes for the initial scan...');
    return await snapshotCreator.createInitialSnapshot();
    
  } catch (error) {
    console.error('‚ùå Error in cached holder data fetch:', error.message);
    
    // Last resort - try Moralis if available
    if (MORALIS_API_KEY) {
      console.log('üîÑ Attempting Moralis API fallback...');
      try {
        return await fetchRealHolderData();
      } catch (moralisError) {
        console.error('‚ùå Moralis fallback failed:', moralisError.message);
      }
    }
    
    // Return basic data if everything fails
    return {
      totalHolders: 0,
      poseidon: 0,
      whale: 0,
      shark: 0,
      dolphin: 0,
      squid: 0,
      shrimp: 0,
      top10Percentage: 0,
      dataSource: 'error',
      error: error.message
    };
  }
}

// Legacy function for compatibility - now uses caching system
async function fetchHolderData() {
  return await fetchHolderDataWithCache();
}

async function runIncrementalUpdate() {
  const IncrementalBurnManager = require('./incremental-burn-manager');
  const manager = new IncrementalBurnManager();
  
  try {
    // Load existing data
    const existingData = manager.loadExistingData();
    
    // Get date range for recent data
    const { startDate, endDate } = manager.getRecentDateRange();
    console.log(`üìÖ Fetching recent data from ${startDate} to ${endDate}...`);
    
    // Fetch recent burn data (modify date range)
    const originalStartDate = new Date();
    originalStartDate.setDate(originalStartDate.getDate() - 30);
    
    // Override the date range for recent data only
    const recentStartDate = new Date(startDate + 'T00:00:00Z');
    const currentBlock = await getBlockNumber();
    const startBlock = await estimateBlockByTimestamp(Math.floor(recentStartDate.getTime() / 1000));
    
    console.log(`üîÑ Fetching recent burns from block ${startBlock} to ${currentBlock}...`);
    
    // Fetch recent burns
    const allBurns = [];
    const totalChunks = Math.ceil((currentBlock - startBlock) / CHUNK_SIZE);
    let chunksProcessed = 0;
    
    console.log(`üîÑ Processing ${totalChunks} chunks for recent data...`);
    
    for (let fromBlock = startBlock; fromBlock <= currentBlock; fromBlock += CHUNK_SIZE) {
      const toBlock = Math.min(fromBlock + CHUNK_SIZE - 1, currentBlock);
      const burns = await fetchBurnsWithRetry(fromBlock, toBlock);
      allBurns.push(...burns);
      chunksProcessed++;
      
      const progress = ((chunksProcessed / totalChunks) * 100).toFixed(1);
      console.log(`üìä Progress: ${chunksProcessed}/${totalChunks} chunks (${progress}%) - Found ${burns.length} burns`);
    }
    
    console.log(`‚úÖ Found ${allBurns.length} recent burn transactions`);
    
    // Process recent burns into daily format (reuse existing logic)
    const burnsByDay = {};
    allBurns.forEach(burn => {
      const date = new Date(burn.timestamp * 1000);
      const dateStr = date.toISOString().split('T')[0];
      
      if (!burnsByDay[dateStr]) {
        burnsByDay[dateStr] = {
          date: dateStr,
          amountTinc: 0,
          transactionCount: 0,
          transactions: []
        };
      }
      
      burnsByDay[dateStr].amountTinc += burn.amount;
      burnsByDay[dateStr].transactionCount++;
      burnsByDay[dateStr].transactions.push({
        hash: burn.hash,
        amount: burn.amount,
        from: burn.from
      });
    });
    
    const recentDailyBurns = Object.values(burnsByDay).sort((a, b) => a.date.localeCompare(b.date));
    
    // Create recent burn data structure
    const recentBurnData = {
      ...existingData,
      dailyBurns: recentDailyBurns,
      holderStats: await fetchHolderData() // Update holder data
    };
    
    // Merge with historical data
    const mergedData = manager.mergeData(existingData, recentBurnData);
    
    // Validate integrity
    manager.validateMergedData(existingData, mergedData);
    
    // Save merged data
    await saveIncrementalData(mergedData);
    
    console.log('‚úÖ Incremental update completed successfully!');
    console.log(`üìä Total TINC burned: ${mergedData.totalBurned.toLocaleString()}`);
    console.log(`üë• Total holders: ${mergedData.holderStats.totalHolders.toLocaleString()}`);
    
  } catch (error) {
    console.error('‚ùå Incremental update failed:', error);
    throw error;
  }
}

async function saveIncrementalData(data) {
  const timestamp = Date.now();
  const versionedFileName = `burn-data-v${timestamp}.json`;
  const dataPath = path.join(__dirname, '../data', versionedFileName);
  const legacyPath = path.join(__dirname, '../data/burn-data.json');
  
  // Write versioned file
  fs.writeFileSync(dataPath, JSON.stringify(data, null, 2));
  
  // Also write legacy file for backward compatibility
  fs.writeFileSync(legacyPath, JSON.stringify(data, null, 2));
  
  // Update version manifest
  const manifestPath = path.join(__dirname, '../data/data-manifest.json');
  const manifest = {
    latest: versionedFileName,
    timestamp: new Date().toISOString(),
    version: timestamp,
    updateType: 'incremental'
  };
  fs.writeFileSync(manifestPath, JSON.stringify(manifest, null, 2));
  
  console.log(`üíæ Incremental data saved to: ${dataPath}`);
}

async function main() {
  try {
    // Check for incremental mode flag
    const isIncremental = process.argv.includes('--incremental');
    
    if (isIncremental) {
      console.log('üîÑ Starting INCREMENTAL data update...');
      return await runIncrementalUpdate();
    } else {
      console.log('üöÄ Starting FULL data refresh...');
      const burnData = await fetchBurnData();
      const holderData = await fetchHolderData();
      
      // Combine burn data with holder data
      const combinedData = {
        ...burnData,
        holderStats: holderData
      };
      
      // Write to versioned data file with timestamp
      const timestamp = Date.now();
      const versionedFileName = `burn-data-v${timestamp}.json`;
      const dataPath = path.join(__dirname, '../data', versionedFileName);
      const legacyPath = path.join(__dirname, '../data/burn-data.json');
      
      // Write versioned file
      fs.writeFileSync(dataPath, JSON.stringify(combinedData, null, 2));
      
      // Also write legacy file for backward compatibility
      fs.writeFileSync(legacyPath, JSON.stringify(combinedData, null, 2));
      
      // Update version manifest
      const manifestPath = path.join(__dirname, '../data/data-manifest.json');
      const manifest = {
        latest: versionedFileName,
        timestamp: new Date().toISOString(),
        version: timestamp,
        updateType: 'full'
      };
      fs.writeFileSync(manifestPath, JSON.stringify(manifest, null, 2));
      
      console.log('‚úÖ Successfully updated burn data!');
      console.log(`üìä Total Supply: ${burnData.totalSupply.toLocaleString()} TINC`);
      console.log(`üî• Total Burned: ${burnData.totalBurned.toLocaleString()} TINC`);
      console.log(`‚ö° Emission Rate: ${burnData.emissionPerSecond.toFixed(4)} TINC/sec`);
      console.log(`üìà Deflationary: ${burnData.isDeflationary ? 'Yes' : 'No'}`);
      console.log(`üë• Total Holders: ${holderData.totalHolders.toLocaleString()}`);
      console.log(`üî± Poseidon (10%+): ${holderData.poseidon}`);
      console.log(`üêã Whale (1%+): ${holderData.whale}`);
      console.log(`üìÖ Data saved to: ${dataPath}`);
    }
    
  } catch (error) {
    console.error('‚ùå Error fetching burn data:', error);
    process.exit(1);
  }
}

if (require.main === module) {
  main();
}

module.exports = { fetchBurnData, callRPC };